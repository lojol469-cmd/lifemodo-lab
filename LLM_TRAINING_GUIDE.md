# ğŸ§  Guide d'EntraÃ®nement LLM - LifeModo AI Lab

## ğŸ¯ **Objectif**
CrÃ©er des **LLMs spÃ©cialisÃ©s par PDF** avec fine-tuning LoRA et export multi-formats.

---

## ğŸ“‹ **FonctionnalitÃ©s**

### âœ… **Mode SÃ©parÃ© LLM**
- **1 LLM par PDF** : Chaque document a son propre modÃ¨le expert
- **Fine-tuning LoRA** : Adaptation lÃ©gÃ¨re (16 rangs) du modÃ¨le de base
- **Quantization 4-bit** : Ã‰conomie de RAM (2-3 GB par modÃ¨le)
- **Export automatique** : ONNX, Safetensors, HuggingFace

---

## ğŸš€ **Utilisation**

### **1. Importer vos PDFs**
```
ğŸ“ Importation DonnÃ©es
â””â”€ Upload PDFs â†’ Lifemodo Lab extrait le texte automatiquement
```

### **2. SÃ©lectionner "Langage (Transformers)"**
```
ğŸ§  EntraÃ®nement IA
â”œâ”€ ModÃ¨les : â˜‘ï¸ Langage (Transformers)
â””â”€ Le systÃ¨me dÃ©tecte les PDFs et propose le mode sÃ©parÃ©
```

### **3. Configurer l'entraÃ®nement**
- **ModÃ¨le de base** : `microsoft/phi-2` (2.7B params, rapide)
  - Alternative : `meta-llama/Llama-3.2-1B`, `mistralai/Mistral-7B-v0.1`
- **Ã‰poques** : 3-5 (plus = meilleure spÃ©cialisation)
- **LoRA Rank** : 16 (Ã©quilibre qualitÃ©/vitesse)

### **4. Lancer l'entraÃ®nement**
```
ğŸš€ Lancer entraÃ®nement
â””â”€ Pour chaque PDF :
    â”œâ”€ Extraction du texte (si non fait)
    â”œâ”€ Fine-tuning LoRA (adapte le modÃ¨le au contenu)
    â”œâ”€ Sauvegarde du modÃ¨le (models/llm_{pdf_name}/)
    â””â”€ Export ONNX automatique (exports/llm_{pdf_name}.onnx)
```

---

## ğŸ“¦ **Formats d'Export**

### **1. HuggingFace Format (DÃ©faut)**
```bash
models/llm_Guide_Word_2013/
â”œâ”€â”€ adapter_config.json       # Config LoRA
â”œâ”€â”€ adapter_model.safetensors # Poids LoRA
â”œâ”€â”€ tokenizer_config.json
â””â”€â”€ tokenizer.model
```

**Utilisation :**
```python
from peft import AutoPeftModelForCausalLM
from transformers import AutoTokenizer

model = AutoPeftModelForCausalLM.from_pretrained("models/llm_Guide_Word_2013")
tokenizer = AutoTokenizer.from_pretrained("models/llm_Guide_Word_2013")

prompt = "Comment insÃ©rer un tableau dans Word 2013 ?"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_length=200)
print(tokenizer.decode(outputs[0]))
```

### **2. ONNX (Production)**
```bash
exports/llm_Guide_Word_2013.onnx  # 5-10 GB
```

**Utilisation avec ONNX Runtime :**
```python
import onnxruntime as ort

session = ort.InferenceSession("exports/llm_Guide_Word_2013.onnx")
# InfÃ©rence rapide cross-platform
```

### **3. Safetensors (SÃ©curisÃ©)**
```bash
models/llm_Guide_Word_2013/adapter_model.safetensors
```
- Format sÃ»r (pas d'exÃ©cution de code arbitraire)
- Compatible HuggingFace Hub

---

## ğŸ“ **ModÃ¨les de Base RecommandÃ©s**

| ModÃ¨le | Taille | RAM Min | Vitesse | Cas d'usage |
|--------|--------|---------|---------|-------------|
| `microsoft/phi-2` | 2.7B | 6 GB | âš¡âš¡âš¡ | IdÃ©al pour dÃ©buter |
| `Qwen/Qwen2.5-1.5B` | 1.5B | 4 GB | âš¡âš¡âš¡âš¡ | Ultra-rapide |
| `meta-llama/Llama-3.2-1B` | 1B | 3 GB | âš¡âš¡âš¡âš¡ | LÃ©ger, efficient |
| `mistralai/Mistral-7B-v0.3` | 7B | 16 GB | âš¡âš¡ | QualitÃ© supÃ©rieure |

---

## ğŸ’¡ **Cas d'Usage**

### **1. Assistant Documentation**
```python
# LLM entraÃ®nÃ© sur "Guide Word 2013"
prompt = "Comment crÃ©er un sommaire automatique ?"
# â†’ RÃ©ponse basÃ©e sur le PDF, pas hallucinations gÃ©nÃ©riques
```

### **2. Chatbot Technique**
```python
# LLM entraÃ®nÃ© sur manuels techniques
prompt = "Erreur #0x80070005 lors de l'installation"
# â†’ Diagnostics prÃ©cis du manuel
```

### **3. GÃ©nÃ©rateur de Contenu**
```python
# LLM entraÃ®nÃ© sur corpus marketing
prompt = "RÃ©dige une description produit pour [...]"
# â†’ Style cohÃ©rent avec la marque
```

---

## âš™ï¸ **Configuration AvancÃ©e**

### **LoRA HyperparamÃ¨tres**
```python
LoraConfig(
    r=16,              # Rank : 8-64 (â†‘ = plus de capacitÃ©)
    lora_alpha=32,     # Scaling : gÃ©nÃ©ralement 2*r
    lora_dropout=0.05, # RÃ©gularisation
    target_modules=["q_proj", "v_proj"]  # Attention layers
)
```

### **Optimisation MÃ©moire**
- **4-bit quantization** : RÃ©duit RAM de 75%
- **Gradient accumulation** : Steps=4 simule batch_size=8
- **FP16 training** : 2x plus rapide

---

## ğŸ› **Troubleshooting**

### **Erreur : "CUDA out of memory"**
```python
# RÃ©duire batch_size
per_device_train_batch_size=1
gradient_accumulation_steps=8
```

### **Erreur : "ImportError: peft not found"**
```bash
pip install peft optimum accelerate bitsandbytes
```

### **ModÃ¨le trop lent**
- Utiliser `microsoft/phi-2` au lieu de Mistral-7B
- Activer `load_in_4bit=True`
- RÃ©duire `max_length=256`

---

## ğŸ“Š **MÃ©triques d'EntraÃ®nement**

Pendant l'entraÃ®nement, surveillez :
- **Loss** : Doit diminuer (< 1.0 = bon)
- **RAM Usage** : Stable sans pics
- **GPU Utilization** : 70-90% optimal

---

## ğŸš€ **DÃ©ploiement**

### **Option 1 : API FastAPI**
```python
from fastapi import FastAPI
from peft import AutoPeftModelForCausalLM

app = FastAPI()
model = AutoPeftModelForCausalLM.from_pretrained("models/llm_Guide_Word_2013")

@app.post("/generate")
def generate(prompt: str):
    outputs = model.generate(tokenizer(prompt, return_tensors="pt").input_ids)
    return {"response": tokenizer.decode(outputs[0])}
```

### **Option 2 : ONNX Runtime**
```python
import onnxruntime as ort
session = ort.InferenceSession("exports/llm_Guide_Word_2013.onnx")
# 3-5x plus rapide que PyTorch !
```

### **Option 3 : HuggingFace Spaces**
```bash
git lfs install
git clone https://huggingface.co/spaces/YOUR_USERNAME/word-assistant
cp -r models/llm_Guide_Word_2013/* word-assistant/
cd word-assistant && git add . && git commit -m "Add model" && git push
```

---

## ğŸ¯ **Prochaines Ã‰tapes**

1. âœ… EntraÃ®ner votre premier LLM sur un PDF
2. âœ… Tester avec des prompts rÃ©els
3. âœ… Exporter en ONNX pour dÃ©ploiement
4. ğŸš€ Publier sur HuggingFace Hub
5. ğŸŒ CrÃ©er une API REST avec FastAPI

---

## ğŸ“š **Ressources**

- [LoRA Paper](https://arxiv.org/abs/2106.09685) - MÃ©thode PEFT
- [Phi-2 Model Card](https://huggingface.co/microsoft/phi-2)
- [ONNX Runtime Docs](https://onnxruntime.ai/docs/)
- [HuggingFace PEFT](https://huggingface.co/docs/peft)

---

**ğŸ‰ Vous Ãªtes maintenant prÃªt Ã  crÃ©er des LLMs experts sur vos propres documents !**
